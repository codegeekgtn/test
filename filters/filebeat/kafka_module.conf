filter {
  #Fields based on https://www.elastic.co/guide/en/beats/filebeat/7.13/filebeat-module-kafka.html (february 2022)
  #https://www.elastic.co/guide/en/beats/filebeat/7.13/exported-fields-kafka.html (february 2022)
  #https://kafka.apache.org/documentation/ version kafka_2.13-3.1.0 (february 2022)
  #and filebeat fields.yml version 7.13.4 oss
  #As the docs says this module work with one event per line, filebeat must ensure to send one event per line.
  
  #Filebeat kafka module, version 2.0.1
  #Filter Input requirements -> fileset: datatype
  #                             log: plain text
  #log: any kind of kafka logs, like: server, controller, log-cleaner and so on

  split {
    field => "message"
    terminator => "<utm-log-separator>"
  }
  json {
    source => "message"
  }

  if ([event][module] and [event][module] == "kafka") 
  or ([service][type] and [service][type] == "kafka") {
    #Generating dataType and dataSource fields
        mutate {
            add_field => { "dataSource" => "%{[host][hostname]}" }
        }
    if [event][module] {
        mutate {
            add_field => { "dataType" => "%{[event][module]}" }
        }
    } else if [service][type] {
        mutate {
            add_field => { "dataType" => "%{[service][type]}" }
        }
    }
    #......................................................................#
    #Parse message field in plain text format
      if [message] {
        grok {
          match => {
              "message" => [
              "\[%{DATA:timestamp}\]%{SPACE}%{WORD:level}%{SPACE}(\[%{DATA:component}\])?(:|,)?%{SPACE}%{GREEDYDATA:msg}%{SPACE}\(%{DATA:class}\)$"
              ]
          } 
        }
      }
    #......................................................................#
    #Then add all possible fields to the json tree structure

        #Fields from kafka plain logs
        mutate {
            #Fields from logs
            rename => { "timestamp" => "[logx][kafka][timestamp]" }
            rename => { "level" => "[logx][kafka][level]" }
            rename => { "component" => "[logx][kafka][component]" }
            rename => { "msg" => "[logx][kafka][msg]" }
            rename => { "class" => "[logx][kafka][class]" }

            #Adding message field
            rename => { "message" => "[logx][kafka][message]" }

            #Adding fields from module
            rename => { "kafka" => "[logx][kafka]" }

        }
        mutate { 
        #General fields from ECS
            rename => { "host" => "[logx][kafka][host]" }
            rename => { "service" => "[logx][kafka][service]" }
            rename => { "ecs" => "[logx][kafka][ecs]" }
            rename => { "agent" => "[logx][kafka][agent]" }
            rename => { "fileset" => "[logx][kafka][fileset]" }
            rename => { "event" => "[logx][kafka][event]" }
            rename => { "input" => "[logx][kafka][input]" }
            rename => { "labels" => "[logx][kafka][labels]" }
            rename => { "as" => "[logx][kafka][as]" }
            rename => { "client" => "[logx][kafka][client]" }
            rename => { "cloud" => "[logx][kafka][cloud]" }
            rename => { "code_signature" => "[logx][kafka][code_signature]" }
            rename => { "container" => "[logx][kafka][container]" }
            rename => { "destination" => "[logx][kafka][destination]" }
            rename => { "dll" => "[logx][kafka][dll]" }
            rename => { "dns" => "[logx][kafka][dns]" }
            rename => { "error" => "[logx][kafka][error]" }
            rename => { "file" => "[logx][kafka][file]" }
            rename => { "geo" => "[logx][kafka][geo]" }
            rename => { "hash" => "[logx][kafka][hash]" }
            rename => { "http" => "[logx][kafka][http]" }
            rename => { "interface" => "[logx][kafka][interface]" }
            rename => { "network" => "[logx][kafka][network]" }
            rename => { "observer" => "[logx][kafka][observer]" }
            rename => { "organization" => "[logx][kafka][organization]" }
            rename => { "package" => "[logx][kafka][package]" }
            rename => { "pe" => "[logx][kafka][pe]" }
            rename => { "process" => "[logx][kafka][process]" }
            rename => { "registry" => "[logx][kafka][registry]" }
            rename => { "related" => "[logx][kafka][related]" }
            rename => { "rule" => "[logx][kafka][rule]" }
            rename => { "server" => "[logx][kafka][server]" }
            rename => { "source" => "[logx][kafka][source]" }
            rename => { "threat" => "[logx][kafka][threat]" }
            rename => { "tls" => "[logx][kafka][tls]" }
            rename => { "span.id" => "[logx][kafka][span.id]" }
            rename => { "trace.id" => "[logx][kafka][trace.id]" }
            rename => { "transaction.id" => "[logx][kafka][transaction.id]" }
            rename => { "url" => "[logx][kafka][url]" }
            rename => { "user" => "[logx][kafka][user]" }
            rename => { "vlan" => "[logx][kafka][vlan]" }
            rename => { "vulnerability" => "[logx][kafka][vulnerability]" }
            rename => { "x509" => "[logx][kafka][x509]" }

        }
        #......................................................................#
        #Finally, remove unnecessary fields
        mutate {
            remove_field => ["@version","tags","log","irrelevant"]
        }
  }
   #Also, remove unwanted fields if the message not match with conditions
   mutate {
      remove_field => ["headers"]
   }
}
